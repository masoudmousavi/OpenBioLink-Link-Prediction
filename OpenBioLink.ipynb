{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenBioLink.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kmgN5oJGLhLT",
        "WWRXIyTVeK93",
        "BYfNTZJFe812",
        "05DipN6ULa5K"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmgN5oJGLhLT"
      },
      "source": [
        "# EXTERNAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FjoSWK_TT4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "884bf728-d0ad-435a-f797-157a03235b66"
      },
      "source": [
        "#@title OpenBioLink Module Install EXTERNAL{ form-width: \"15%\" }\n",
        "! pip install openbiolink"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openbiolink\n",
            "  Downloading openbiolink-0.1.4-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30 kB 40.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 40 kB 42.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 51 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 61 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 71 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 81 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 102 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 112 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 122 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 133 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 143 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 153 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 163 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 174 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 194 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 204 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 215 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 225 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 226 kB 29.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from openbiolink) (7.1.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from openbiolink) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openbiolink) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->openbiolink) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.4->openbiolink) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->openbiolink) (3.10.0.2)\n",
            "Installing collected packages: openbiolink\n",
            "Successfully installed openbiolink-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyVfad59VpsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1b1984-1c13-42aa-9dc3-c1ff8a04c0a2"
      },
      "source": [
        "#@title PyG 113 Installation EXTERNAL{ form-width: \"15%\" }\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 29.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 29.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.9 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWRXIyTVeK93"
      },
      "source": [
        "# HEADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIDIxZaJTcK2"
      },
      "source": [
        "#@title Module Imports HEADER { form-width: \"15%\" }\n",
        "from openbiolink.obl2021 import OBL2021Dataset, OBL2021Evaluator\n",
        "import torch\n",
        "from torch.nn import Module,\\\n",
        "                     ModuleList,\\\n",
        "                     Embedding,\\\n",
        "                     BatchNorm1d,\\\n",
        "                     Linear,\\\n",
        "                      BCEWithLogitsLoss,\\\n",
        "                     CrossEntropyLoss,\\\n",
        "                     Dropout\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as PyG\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn.conv import RGCNConv\n",
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from typing import NoReturn\n",
        "import typing\n",
        "import time\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3doXenteXFa"
      },
      "source": [
        "# GLOBAL \n",
        "<!-- ###now CPU -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-0w2buqhHoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43de108a-7686-4a38-95a3-a5582ff370b6"
      },
      "source": [
        "#@title Global Variables CLASS{ form-width: \"15%\" }\n",
        "class Global(Enum):\n",
        "  HEAD_INDEX = 0\n",
        "  RELATION_INDEX = 1\n",
        "  TAIL_INDEX = 2\n",
        "  FEATURE_ENG = 'one-hot'\n",
        "  NUM_RELATIONS = 28\n",
        "  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  MINI_BATCH_SIZE = 32\n",
        "  DISP_HITS = False\n",
        "  MANUAL_SEED = 2021\n",
        "\n",
        "\n",
        "torch.manual_seed(Global.MANUAL_SEED.value)\n",
        "dataset = OBL2021Dataset()\n",
        "evaluator = OBL2021Evaluator()\n",
        "kg = torch.cat((dataset.training, dataset.validation, dataset.testing), dim=0)\n",
        "sorted_on_tails_indcs_kg = torch.sort(kg[:, Global.TAIL_INDEX.value])[1]\n",
        "kg_sorted_tails = kg[sorted_on_tails_indcs_kg]\n",
        "sorted_on_tails_indcs_kg = None\n",
        "sorted_on_heads_indcs_kg = torch.sort(kg[:, Global.HEAD_INDEX.value])[1]\n",
        "kg_sorted_heads = kg[sorted_on_heads_indcs_kg]\n",
        "sorted_on_heads_indcs_kg = None\n",
        "features = dataset.candidates.to(Global.DEVICE.value) \n",
        "train_set = dataset.training.to(Global.DEVICE.value) # torch.tensor of shape(num_train,3)\n",
        "val_set = dataset.validation.to(Global.DEVICE.value) # torch.tensor of shape(num_val,3)\n",
        "test_set = dataset.testing.to(Global.DEVICE.value)   # torch.tensor of shape(num_train,3)\n",
        "dataset = None\n",
        "sorted_train_set = train_set[torch.sort(train_set[:, 0])[1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset not found, downloading to /content/obl2021 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "KGID_HQ_DIR.zip: 45.2MB [00:04, 9.92MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQs2s9trcsZ",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# times = list()\n",
        "# for h in tqdm(features):\n",
        "#   head_times = find_start_and_end_indcs(h, 0, train_set[torch.sort(train_set[:, 0])[1]])\n",
        "#   if isinstance(head_times, torch.Tensor):\n",
        "#     times.append(head_times.shape[0])\n",
        "#   else:\n",
        "#     times.append(0)\n",
        "# torch.tensor(times)\n",
        "head_entities_in_train_set = torch.unique(torch.sort(train_set[:, 0])[0])\n",
        "tail_entities_in_train_set = torch.unique(torch.sort(train_set[:, 2])[0])\n",
        "head_entities_in_val_set = torch.unique(torch.sort(val_set[:, 0])[0])\n",
        "tail_entities_in_val_set = torch.unique(torch.sort(val_set[:, 2])[0])\n",
        "head_entities_in_test_set = torch.unique(torch.sort(test_set[:, 0])[0])\n",
        "tail_entities_in_test_set = torch.unique(torch.sort(test_set[:, 2])[0])\n",
        "\n",
        "# head_entities_in_train_set[head_entities_in_val_set[:10]] == head_entities_in_train_set[:10]\n",
        "\n",
        "print(f'#Heads at train set: {head_entities_in_train_set.shape[0]}')\n",
        "print(f'#Tails at train set: {tail_entities_in_train_set.shape[0]}')\n",
        "print(f'#Heads at val set:   {head_entities_in_val_set.shape[0]}')\n",
        "print(f'#Tails at val set:   {tail_entities_in_val_set.shape[0]}')\n",
        "print(f'#Heads at test set:  {head_entities_in_test_set.shape[0]}')\n",
        "print(f'#Tails at test set:  {tail_entities_in_test_set.shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYfNTZJFe812"
      },
      "source": [
        "# GNN Model and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EDUweUw5uvi",
        "cellView": "form"
      },
      "source": [
        "#@title GNN Model CLASS{ form-width: \"10%\" }\n",
        "class GNN(Module):\n",
        "  def __init__(self, conv_dims: list, fully_connected_for_head_dims: list, fully_connected_for_tail_dims: list, x_feature:str, dropout: dict, embedding_dims: tuple=None)-> NoReturn:\n",
        "    super(GNN, self).__init__()\n",
        "    self.conv_dropout = Dropout(p=dropout[\"conv\"])\n",
        "    self.emb_dropout = Dropout(p=dropout[\"emb\"]) \n",
        "    self.fc_dropout = Dropout(p=dropout[\"fc\"])\n",
        "    self.x_feature = x_feature\n",
        "    self.num_relations = Global.NUM_RELATIONS.value\n",
        "    self.relation_embedder = Embedding(self.num_relations, fully_connected_for_tail_dims[-1])\n",
        "    if x_feature == 'one-hot':\n",
        "      self.entity_embedder = Embedding(embedding_dims[0], embedding_dims[1])\n",
        "      first_conv_layer = [RGCNConv(embedding_dims[1], conv_dims[0], self.num_relations)]\n",
        "    elif x_feature == 'identity':\n",
        "      first_conv_layer = [RGCNConv(1, conv_dims[0], self.num_relations)]\n",
        "    conv_list = first_conv_layer + \\\n",
        "                                [\n",
        "                                  RGCNConv(conv_dims[i], conv_dims[i+1], self.num_relations)\n",
        "                                  for i in range(len(conv_dims[:-1]))\n",
        "                                ]\n",
        "  \n",
        "    fully_connected_list_for_tail = [\n",
        "                                      Linear(conv_dims[-1], fully_connected_for_tail_dims[0]) \n",
        "                                    ] +\\\n",
        "                                    [\n",
        "                                     Linear(fully_connected_for_tail_dims[i], fully_connected_for_tail_dims[i + 1])\n",
        "                                     for i in range(len(fully_connected_for_tail_dims[:-1]))\n",
        "                                    ]\n",
        "\n",
        "    fully_connected_list_for_head = [\n",
        "                                      Linear(conv_dims[-1], fully_connected_for_head_dims[0]) \n",
        "                                    ] +\\\n",
        "                                    [\n",
        "                                     Linear(fully_connected_for_head_dims[i], fully_connected_for_head_dims[i + 1])\n",
        "                                     for i in range(len(fully_connected_for_head_dims[:-1]))\n",
        "                                    ]\n",
        "    \n",
        "\n",
        "    #graph conv layers\n",
        "    self.conv_layers = ModuleList(conv_list)\n",
        "\n",
        "    #fully connected dense layers\n",
        "    self.head_fc_layers = ModuleList(fully_connected_list_for_head)\n",
        "    self.tail_fc_layers = ModuleList(fully_connected_list_for_tail)\n",
        "    \n",
        "\n",
        "  def reset_parameters(self):\n",
        "    self.entity_embedder.reset_parameters()\n",
        "    self.relation_embedder.reset_parameters()\n",
        "    for conv in self.conv_layers:\n",
        "        conv.reset_parameters()\n",
        "    for fc in self.head_fc_layers:\n",
        "        fc.reset_parameters()\n",
        "    for fc in self.tail_fc_layers:\n",
        "        fc.reset_parameters()\n",
        "\n",
        "\n",
        "  def forward(self, data: Data) -> torch.Tensor:\n",
        "    edge_index = data.edge_index_messaging\n",
        "    x = data.x\n",
        "    edge_type = data.edge_type_messaging\n",
        "\n",
        "    ####################################### One-Hot Entity Encoder #######################################\n",
        "    if self.x_feature == 'one-hot':\n",
        "      x = self.entity_embedder(x).reshape(self.entity_embedder.weight.shape[0], -1)\n",
        "      \n",
        "    ############################################## Identity Encoder ################################################\n",
        "    elif self.x_feature == 'identity':\n",
        "      x = torch.ones(x.shape[0], 1)\n",
        "      if self.training:\n",
        "        x = self.emb_dropout(x)\n",
        "    ####################################### RGCN Encoder #######################################\n",
        "    for conv in self.conv_layers[:-1]:\n",
        "      x = conv(x, edge_index=edge_index, edge_type=edge_type)\n",
        "      x = F.relu(x)\n",
        "      if self.training:\n",
        "        x = self.conv_dropout(x)\n",
        "    x = self.conv_layers[-1](x, edge_index, edge_type)\n",
        "    if self.training:\n",
        "      x = self.conv_dropout(x)\n",
        "    \n",
        "    ####################################### Decoder #######################################\n",
        "    positive_heads = x[data.edge_index_supervision[0]]\n",
        "    positive_tails = x[data.edge_index_supervision[1]]\n",
        "    positive_relations = self.relation_embedder(data.edge_type_supervision) # relation encoder\n",
        "\n",
        "    negative_heads = x[data.edge_index_negative[0]]\n",
        "    negative_tails = x[data.edge_index_negative[1]]\n",
        "    negative_relations = self.relation_embedder(data.edge_type_negative)\n",
        "    if self.training:\n",
        "      negative_relations = self.emb_dropout(negative_relations)\n",
        "      positive_relations = self.emb_dropout(positive_relations)\n",
        "\n",
        "    for head_fc, tail_fc in zip(self.head_fc_layers, self.tail_fc_layers):\n",
        "      positive_heads = head_fc(positive_heads)\n",
        "      positive_heads = F.relu(positive_heads)\n",
        "\n",
        "      negative_heads = head_fc(negative_heads)\n",
        "      negative_heads = F.relu(negative_heads)\n",
        "      \n",
        "      positive_tails = tail_fc(positive_tails)\n",
        "      positive_tails = F.relu(positive_tails) \n",
        "\n",
        "      negative_tails = tail_fc(negative_tails)\n",
        "      negative_tails = F.relu(negative_tails)\n",
        "\n",
        "      if self.training:\n",
        "        positive_heads = self.conv_dropout(positive_heads)\n",
        "        negative_heads = self.conv_dropout(negative_heads)\n",
        "        positive_tails = self.conv_dropout(positive_tails)\n",
        "        negative_tails = self.conv_dropout(negative_tails)\n",
        "    \n",
        "    # let f be the fully connected transform function for heads, and \n",
        "    # let g be the fully connected transform function for tails:\n",
        "        # prior to the fully connected transforms:\n",
        "        #     score(head, relation, tail) = <head, relation, tail> = head . relation . tail\n",
        "        #     score(tail, relation, head) = <tail, relation, head> = tail . relation . head\n",
        "        #     ===> score(head, relation, tail) = score(tail, relation, head) essentially holds\n",
        "        # now with the fully connected transforms:\n",
        "        #     score(head, relation, tail) = <f(head), relation, g(tail)> = f(head) . relation . g(tail)\n",
        "        #     score(tail, relation, head) = <f(tail), relation, g(head)> = f(tail) . relation . g(head)\n",
        "        #     ===> score(head, relation, tail) = score(tail, relation, head) does not necessarily holds\n",
        "\n",
        "    return (positive_heads * positive_relations * positive_tails).sum(dim=1), \\\n",
        "           (negative_heads * negative_relations * negative_tails).sum(dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq9CoeHZ7SHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "aaf2c31e-c5ad-4148-c6f0-c62e4c9b675e"
      },
      "source": [
        "#@title Model and Hyperparameters MAIN { form-width: \"15%\" }\n",
        "model = GNN(\n",
        "    x_feature='one-hot',\n",
        "    conv_dims=[32, 32, 32, 32],\n",
        "    embedding_dims=(features.max() + 1, 32),\n",
        "    fully_connected_for_head_dims=[32, 32], \n",
        "    fully_connected_for_tail_dims=[32, 32],\n",
        "    dropout={\n",
        "        \"emb\": 0.2,\n",
        "        \"conv\": 0.2,\n",
        "        \"fc\": 0.5\n",
        "    }\n",
        ").to(Global.DEVICE.value)\n",
        "print(model)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "opt = Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv_dropout): Dropout(p=0.2, inplace=False)\n",
            "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relation_embedder): Embedding(28, 32)\n",
            "  (entity_embedder): Embedding(180992, 32)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): RGCNConv(32, 32, num_relations=28)\n",
            "    (1): RGCNConv(32, 32, num_relations=28)\n",
            "    (2): RGCNConv(32, 32, num_relations=28)\n",
            "    (3): RGCNConv(32, 32, num_relations=28)\n",
            "  )\n",
            "  (head_fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "  )\n",
            "  (tail_fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2vzpaRVecRw"
      },
      "source": [
        "#FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87ZALUBYec9",
        "cellView": "form"
      },
      "source": [
        "#@title find_start_and_end_indcs() PRIVATE-FUNCTION { form-width: \"15%\" }\n",
        "def find_start_and_end_indcs(entity, entity_type, kg_sorted):\n",
        "  up = kg_sorted.shape[0]\n",
        "  down = 0\n",
        "  indx = kg_sorted.shape[0] // 2\n",
        "  found = False\n",
        "  while up - down > 1:\n",
        "    if kg_sorted[indx][entity_type].item() == entity:\n",
        "      found = True\n",
        "      break \n",
        "    elif kg_sorted[indx][entity_type].item() >= entity: \n",
        "      up = indx\n",
        "      indx = (up + down) // 2\n",
        "    else:\n",
        "      down = indx \n",
        "      indx = (up + down) // 2\n",
        "  if not found:\n",
        "    return None\n",
        "  while 1:\n",
        "    indx += 1\n",
        "    try:\n",
        "      if not kg_sorted[indx][entity_type].item() == entity:\n",
        "        indx -= 1\n",
        "        end_indx = indx\n",
        "        break\n",
        "    except:\n",
        "      end_indx = indx - 1\n",
        "      break\n",
        "  \n",
        "\n",
        "  while 1:\n",
        "    indx -= 1\n",
        "    try:\n",
        "      if not kg_sorted[indx][entity_type].item() == entity:\n",
        "        indx += 1\n",
        "        start_indx = indx\n",
        "        break\n",
        "    except:\n",
        "      start_indx = indx + 1\n",
        "      break\n",
        "  return torch.tensor(range(start_indx, end_indx + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "C-bokmDVpe68"
      },
      "source": [
        "#@title check_negative_samples() FUNCTION { form-width: \"15%\" }\n",
        "def check_negative_samples(negative_samples, sorted_train_set):\n",
        "  for triple in negative_samples:\n",
        "    indcs = find_start_and_end_indcs(triple[0], 0, sorted_train_set)\n",
        "    if isinstance(indcs, torch.Tensor):\n",
        "      facts = sorted_train_set[indcs]\n",
        "      are_actually_corrupt = ((triple == facts).type(torch.int8).sum(dim=-1) < 3).sum()\n",
        "      if not are_actually_corrupt.type(torch.int8):\n",
        "        return False \n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYPeOqPxkFtD",
        "cellView": "form"
      },
      "source": [
        "#@title get_psuedo_negative_entities() FUNCTIONS { form-width: \"15%\" }\n",
        "def get_psuedo_negative_entities(entity, corrupt_at, kg_sorted):\n",
        "  if corrupt_at == Global.HEAD_INDEX.value:\n",
        "    entity_type = Global.TAIL_INDEX.value \n",
        "  elif corrupt_at == Global.TAIL_INDEX.value:\n",
        "    entity_type = Global.HEAD_INDEX.value\n",
        "\n",
        "  indcs = find_start_and_end_indcs(entity, entity_type, kg_sorted=kg_sorted)\n",
        "  \n",
        "  if indcs is not None:\n",
        "    fact_triplets_entities = kg_sorted[indcs][:, corrupt_at]\n",
        "    features_copy = features.detach().clone()\n",
        "    features_copy[fact_triplets_entities] = -1\n",
        "    non_negative_mask = features_copy >= 0\n",
        "    ret = torch.nonzero(non_negative_mask).reshape(-1)\n",
        "    return ret\n",
        "  else:\n",
        "    return features.to(Global.DEVICE.value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WweaTsWIf9SI"
      },
      "source": [
        "\n",
        "#@title #TODO mini_batch_maker NEGATIVE SAMPLING use MSCaching FUNCTION  { form-width: \"15%\" }\n",
        "def mini_batch_maker(messaging, supervision, candidates, x_feature='one-hot'):\n",
        "  heads = supervision[:, 0]\n",
        "  relations = supervision[:, 1]\n",
        "  tails = supervision[:, -1]\n",
        "\n",
        "  ct_size = supervision.shape[0] // 2\n",
        "  ch_size = supervision.shape[0] - ct_size\n",
        "  while 1:\n",
        "    negative_samples_corrupted_tails = torch.vstack(\n",
        "        (\n",
        "            heads[: ct_size], \n",
        "            relations[: ct_size], \n",
        "            torch.multinomial(\n",
        "                candidates.type(torch.float).to(Global.DEVICE.value), \n",
        "                ct_size\n",
        "            )\n",
        "        )\n",
        "    ).t().contiguous()\n",
        "\n",
        "    negative_samples_corrupted_heads = torch.vstack(\n",
        "        ( \n",
        "            torch.multinomial(\n",
        "                candidates.type(torch.float).to(Global.DEVICE.value), \n",
        "                ch_size\n",
        "            ), \n",
        "            relations[ch_size:], \n",
        "            tails[ch_size:]\n",
        "        )\n",
        "    ).t().contiguous()\n",
        "\n",
        "    negative_samples = torch.cat(\n",
        "        (negative_samples_corrupted_heads, negative_samples_corrupted_tails),\n",
        "        dim=0\n",
        "    )\n",
        "    if check_negative_samples(negative_samples, sorted_train_set):\n",
        "      break\n",
        "\n",
        "\n",
        "  # relations = supervision[:, 1]\n",
        "  # while True:\n",
        "  #   batch_size = supervision.shape[0]\n",
        "  #   negative_samples = torch.vstack(\n",
        "  #       (\n",
        "  #           torch.multinomial(candidates.type(torch.float).to(Global.DEVICE.value), batch_size),\n",
        "  #           relations,\n",
        "  #           torch.multinomial(candidates.type(torch.float).to(Global.DEVICE.value), batch_size)\n",
        "  #       )\n",
        "  #   ).t().contiguous()\n",
        "  #   if check_negative_samples(negative_samples, sorted_train_set):\n",
        "  #     break\n",
        "\n",
        "  graph = graph_data_maker(\n",
        "      messaging=messaging,\n",
        "      supervision=supervision,\n",
        "      negative_samples=negative_samples,\n",
        "      x=candidates.to(Global.DEVICE.value),\n",
        "      x_feature='one-hot'\n",
        "  )\n",
        "\n",
        "  return graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI94NPZcHanb",
        "cellView": "form"
      },
      "source": [
        "#@title graph_data_maker(dataset, x_feature) FUNCTION{ form-width: \"15%\" }\n",
        "def graph_data_maker(messaging: torch.Tensor, supervision: torch.Tensor, negative_samples: torch.Tensor, x:torch.Tensor, x_feature: str=Global.FEATURE_ENG.value, check_for_correctness: bool=False) -> Data:\n",
        "  relation_idx = Global.RELATION_INDEX.value\n",
        "  head_idx = Global.HEAD_INDEX.value\n",
        "  tail_idx = Global.TAIL_INDEX.value\n",
        "  graph_data = Data(\n",
        "        x=x.reshape(-1, 1),\n",
        "        edge_index_messaging=messaging[:, (head_idx, tail_idx)].t().contiguous(),\n",
        "        edge_type_messaging=messaging[:, relation_idx],\n",
        "        edge_index_supervision=supervision[:, (head_idx, tail_idx)].t().contiguous(),\n",
        "        edge_type_supervision=supervision[:, relation_idx],\n",
        "        edge_index_negative=negative_samples[:, (head_idx, tail_idx)].t().contiguous(),\n",
        "        edge_type_negative=negative_samples[:, (relation_idx)]\n",
        "    )\n",
        "  return graph_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbO6PxCfnuiD",
        "cellView": "form"
      },
      "source": [
        "#@title train(model, graph, optimizer, loss_fn) FUNCTION{ form-width: \"15%\" }\n",
        "def train(model: GNN, graph: Data, optimizer: torch.optim, loss_fn:torch.nn.modules.loss) -> tuple((torch.float, torch.Tensor, torch.Tensor)):\n",
        "  optimizer.zero_grad()\n",
        "  model.train()\n",
        "  positive, negative = model(graph)\n",
        "  loss = loss_fn(\n",
        "            negative, \n",
        "            torch.zeros(graph.edge_index_negative.shape[1]).to(Global.DEVICE.value)\n",
        "         ) + loss_fn(\n",
        "                positive,\n",
        "                torch.ones(graph.edge_index_supervision.shape[1]).to(Global.DEVICE.value)\n",
        "            )\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item(), torch.sigmoid(positive), torch.sigmoid(negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcSjKWmGyYuj",
        "cellView": "form"
      },
      "source": [
        "#@title evaluation_rank() PRIVATE-FUNCTION { form-width: \"10%\" }\n",
        "@torch.no_grad()\n",
        "def evaluation_rank(model, eval_set, messaging_set):\n",
        "  h_pred_top10 = list()\n",
        "  t_pred_top10 = list()\n",
        "\n",
        "  for eval_triplet in eval_set:\n",
        "    #use tqdm\n",
        "    head = eval_triplet[Global.HEAD_INDEX.value].item()\n",
        "    relation = eval_triplet[Global.RELATION_INDEX.value].item()\n",
        "    tail = eval_triplet[Global.TAIL_INDEX.value].item()\n",
        "\n",
        "    corrupted_tails = get_psuedo_negative_entities(\n",
        "        entity=head, \n",
        "        corrupt_at=Global.TAIL_INDEX.value, \n",
        "        kg_sorted=kg_sorted_heads\n",
        "    )\n",
        "    \n",
        "    num_psuedo_negative_triples = corrupted_tails.shape[0]\n",
        "    psuedo_negative_triplets_corrupted_tail = torch.vstack(\n",
        "        (\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * head,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * relation,\n",
        "            corrupted_tails\n",
        "        )\n",
        "    ).t().to(Global.DEVICE.value)\n",
        "\n",
        "    corrupted_heads = get_psuedo_negative_entities(\n",
        "        entity=tail, \n",
        "        corrupt_at=Global.HEAD_INDEX.value, \n",
        "        kg_sorted=kg_sorted_tails\n",
        "    )\n",
        "\n",
        "    num_psuedo_negative_triples = corrupted_heads.shape[0]\n",
        "    psuedo_negative_triplets_corrupted_head = torch.vstack(\n",
        "        (\n",
        "            corrupted_heads,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * relation,\n",
        "            torch.ones(num_psuedo_negative_triples).to(Global.DEVICE.value).type(torch.long) * tail\n",
        "        )\n",
        "    ).t().to(Global.DEVICE.value)\n",
        "\n",
        "    # eval_triplet: (h, r, t)\n",
        "    # psuedo_negative_triplets_head: (h′, r, t) for all h′\n",
        "    # psuedo_negative_triplets_tail: (h, r, t′) for all t′\n",
        "\n",
        "    # train_set being the messaging graph, calculate the score for (h, r, t)\n",
        "    # train_set being the messaging graph, calculate the scores for all (h′, r, t)\n",
        "    # train_set being the messaging graph, calculate the scores for all (h, r, t′)\n",
        "\n",
        "    graph_data_for_object_tail = graph_data_maker(\n",
        "      messaging=messaging_set,\n",
        "      supervision=eval_triplet.reshape(1, 3),\n",
        "      negative_samples=psuedo_negative_triplets_corrupted_head,\n",
        "      x=features.to(Global.DEVICE.value),\n",
        "      x_feature='one-hot'\n",
        "    )\n",
        "\n",
        "    graph_data_for_object_head = graph_data_maker(\n",
        "            messaging=messaging_set,\n",
        "            supervision=eval_triplet.reshape(1, 3),\n",
        "            negative_samples=psuedo_negative_triplets_corrupted_tail,\n",
        "            x=features.to(Global.DEVICE.value),\n",
        "            x_feature='one-hot'\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    scores_for_object_tail = torch.cat(model(graph_data_for_object_tail))\n",
        "\n",
        "    sorted_by_scores_for_object_tail_indcs = torch.sort(scores_for_object_tail, descending=True)[1]\n",
        "\n",
        "    head_objects = torch.cat((graph_data_for_object_tail.edge_index_supervision[0], graph_data_for_object_tail.edge_index_negative[0]))\n",
        "    top10_heads = head_objects[sorted_by_scores_for_object_tail_indcs[:10]]\n",
        "    h_pred_top10.append(top10_heads)\n",
        "\n",
        "    model.eval()\n",
        "    scores_for_object_head = torch.cat(model(graph_data_for_object_head))\n",
        "\n",
        "    sorted_by_scores_for_object_head_indcs = torch.sort(scores_for_object_head, descending=True)[1]\n",
        "\n",
        "    tail_objects = torch.cat((graph_data_for_object_head.edge_index_supervision[1], graph_data_for_object_head.edge_index_negative[1]))\n",
        "    top10_tails = tail_objects[sorted_by_scores_for_object_head_indcs[:10]]\n",
        "    t_pred_top10.append(top10_tails)\n",
        "\n",
        "    # print('')\n",
        "    ############################################################################## 1\n",
        "    # if False:\n",
        "    #   hit_index = (\n",
        "    #             tail_objects[sorted_by_scores_for_object_head_indcs[:500]] == eval_triplet[2]\n",
        "    #           ).nonzero(as_tuple=True)[0] + 1\n",
        "    #   if hit_index.shape[0]:\n",
        "    #     print(f'Tail ranked at {hit_index.item()}')\n",
        "    #     print('-' * 30)\n",
        "    # ############################################################################### 2\n",
        "\n",
        "    # ############################################################################### 3\n",
        "    # hit_index = (\n",
        "    #             head_objects[sorted_by_scores_for_object_tail_indcs[:500]] == eval_triplet[0]\n",
        "    #           ).nonzero(as_tuple=True)[0] + 1\n",
        "    # if hit_index.shape[0]:\n",
        "    #   print(f'Head ranked at {hit_index.item()}')\n",
        "    #   print('-' * 30)\n",
        "    ############################################################################## 4\n",
        "    \n",
        "  model.train()\n",
        "  return torch.stack(h_pred_top10), torch.stack(t_pred_top10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzR8BWA0N628",
        "cellView": "form"
      },
      "source": [
        "#@title evaluate_hits_at_10(model, mode) FUNCTION{ form-width: \"15%\" }\n",
        "def evaluate_hits_at_10(model: GNN, mode:str) -> torch.float:\n",
        "  if mode == 'validation':\n",
        "    head_objects_top10, tail_objects_top10 = evaluation_rank(model, val_set, train_set)\n",
        "    return evaluator.eval(\n",
        "            head_objects_top10,\n",
        "            tail_objects_top10,\n",
        "            val_set,\n",
        "            False\n",
        "          )\n",
        "  elif mode == 'testing':\n",
        "    head_objects_top10, tail_objects_top10 = evaluation_rank(model, test_set, torch.cat((train_set, val_set), dim=0))\n",
        "    return evaluator.eval(\n",
        "            head_objects_top10,\n",
        "            tail_objects_top10,\n",
        "            test_set,\n",
        "            False\n",
        "          )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BkpBdADKJqTh"
      },
      "source": [
        "#@title save_execution() FUNCTION { form-width: \"15%\" }\n",
        "def save_execution(model, iteration, epoch, epoch_loss):\n",
        "  saved_time = str(datetime.now())\n",
        "  torch.save(model, f'model@{saved_time}.pth')\n",
        "  with open(f'info@{saved_time}.txt', 'w') as info_file:\n",
        "    info_file.write(f'model: {str(model)}')\n",
        "    info_file.write('\\n')\n",
        "    info_file.write(f'iteration: {iteration}\\n')\n",
        "    info_file.write(f'epoch: {epoch + 1}\\n')\n",
        "    info_file.write(f'epoch loss: {epoch_loss}')\n",
        "\n",
        "  # from google.colab import drive\n",
        "  # drive.mount('/content/drive')\n",
        "  # import shutil\n",
        "  # shutil.copy(f'/content/model@{saved_time}.pth', f'/content/drive/model@{saved_time}.pth')\n",
        "  # shutil.copy(f'/content/info@{saved_time}.txt', f'/content/drive/info@{saved_time}.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7j9JFT2fLCW"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5SkTuG_8R1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6badb9d9-99cd-4f98-944c-6c6dc9b85434"
      },
      "source": [
        "# @title Parameter and Hyperparameter Tuning MAIN{ form-width: \"10%\" }\n",
        "model.reset_parameters()\n",
        "try:\n",
        "  for epoch in range(5):\n",
        "    iteration = 0\n",
        "    epoch_loss = 0\n",
        "  ############################## TRAIN Graph Maker #####################################\n",
        "    for i in tqdm(range(0, train_set.shape[0] - Global.MINI_BATCH_SIZE.value, Global.MINI_BATCH_SIZE.value)):\n",
        "      iteration += 1\n",
        "      train_supervision = train_set[i: i + Global.MINI_BATCH_SIZE.value, :]\n",
        "      train_messaging = torch.cat(\n",
        "          (train_set[: i, :], train_set[i + Global.MINI_BATCH_SIZE.value: , :]),\n",
        "          dim=0\n",
        "      )\n",
        "      train_graph = mini_batch_maker(\n",
        "          train_messaging,\n",
        "          train_supervision,\n",
        "          features,\n",
        "          'one-hot'\n",
        "      )\n",
        "      loss, positive_score, negative_score = train(model, train_graph, opt, loss_fn)\n",
        "      epoch_loss += loss\n",
        "      if iteration % 50 == 0:\n",
        "        print('')\n",
        "        print('-' * 50)\n",
        "        agg_p_score = positive_score.sum().item()\n",
        "        agg_n_score = negative_score.sum().item()\n",
        "        print(f'Train Batch {iteration}:')\n",
        "        print('-' * 50)\n",
        "        print(f'Batch Loss:    {loss: .4f}')\n",
        "        print('-' * 50)\n",
        "        print(f'Average Loss:         <{epoch_loss / iteration: .4f} >')\n",
        "        print('-' * 50)\n",
        "        print(f'Agg P-Score:   {agg_p_score: .4f}')\n",
        "        print('-' * 50)\n",
        "        print(f'Agg N-Score:   {agg_n_score: .4f}')\n",
        "        print(f'===' * 25)\n",
        "\n",
        "        ###########################################\n",
        "        a, b = evaluation_rank(model, val_set[1000:1100], train_set)\n",
        "        evaluator.eval(\n",
        "                    a,\n",
        "                    b,\n",
        "                    val_set[1000:1100],\n",
        "                    False\n",
        "                  )\n",
        "        print(f'***' * 25)\n",
        "        ###########################################\n",
        "      if iteration % 1500 == 0:\n",
        "        save_execution(model, iteration, epoch, epoch_loss)\n",
        "\n",
        "    evaluate_hits_at_10(model, mode='validation')\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "  # save_execution(model, iteration, epoch, epoch_loss)\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/131000 [00:31<1133:36:35, 31.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs-CPM6MflBB"
      },
      "source": [
        "# TEST EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kadd-g_sb5kB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "abe8638b-bc27-4469-d83c-7de1786e105f"
      },
      "source": [
        "a, b = evaluation_rank(model, val_set[1000:1500], train_set)\n",
        "evaluator.eval(\n",
        "            a,\n",
        "            b,\n",
        "            val_set[1000:1200],\n",
        "            False\n",
        "          )\n",
        "\n",
        "# (h, r, t) (h, r, t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 6/500 [00:06<08:44,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-06c9472c7e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m evaluator.eval(\n\u001b[1;32m      3\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mval_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-3dd9a392af37>\u001b[0m in \u001b[0;36mevaluation_rank\u001b[0;34m(model, eval_set, messaging_set)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mscores_for_object_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data_for_object_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0msorted_by_scores_for_object_head_indcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_for_object_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b4bdb76786a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m####################################### RGCN Encoder #######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/rgcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No regularization/Basis-decomposition ========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/rgcn_conv.py\u001b[0m in \u001b[0;36mmasked_edge_index\u001b[0;34m(edge_index, edge_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmasked_edge_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmasked_select_nnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHH3QX0W6blV"
      },
      "source": [
        "try:\n",
        "  evaluate_hits_at_10(model, 'testing')\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DipN6ULa5K"
      },
      "source": [
        "# ADD-ONS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqWPI4Bh_Q1",
        "cellView": "form"
      },
      "source": [
        "#@title add_tail_to_head(data_split) FUNCTION { form-width: \"15%\" }\n",
        "def add_tail_to_head(data_split: torch.Tensor, plus: int=28) -> torch.Tensor:\n",
        "  heads = data_split[:, (0)]\n",
        "  tails = data_split[:, (-1)]\n",
        "  relations = data_split[:, (1)]\n",
        "  \n",
        "  tail_to_head = torch.vstack(\n",
        "      (tails, relations + plus, heads)\n",
        "  ).t()\n",
        "\n",
        "  return torch.cat(\n",
        "      (data_split, tail_to_head),\n",
        "      dim=0\n",
        "  )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV0mkcK1rMhq",
        "cellView": "form"
      },
      "source": [
        "#@title checksum(data_subset, graph_data) FUNCTION { form-width: \"15%\" }\n",
        "def checksum(data_subset: torch.Tensor, graph_data: Data, step: int=500) -> NoReturn:\n",
        "  for i in range(step, data_subset.shape[0], step):\n",
        "    graph_head_indcs = graph_data.edge_index[0, i-step:i]\n",
        "    data_head_entities = data_subset[i-step:i, 0]\n",
        "    graph_head_entities = graph_data.x[graph_head_indcs].reshape(-1)\n",
        "\n",
        "    checksum_head = (graph_head_entities == data_head_entities).sum().item()\n",
        "    if not checksum_head == step:\n",
        "      print('head')\n",
        "      print(i)\n",
        "      break\n",
        "\n",
        "    graph_relation_indcs = graph_data.edge_type[i-step:i]\n",
        "    data_relation_types = data_subset[i-step:i, 1]\n",
        "\n",
        "    checksum_relation = (graph_relation_indcs == data_relation_types).sum().item()\n",
        "    if not checksum_relation == step:\n",
        "      print('relation')\n",
        "      print(i)\n",
        "      break\n",
        "\n",
        "    graph_tail_indcs = graph_data.edge_index[1, i-step:i]\n",
        "    data_tail_entities = data_subset[i-step:i, 2]\n",
        "    graph_tail_entities = graph_data.x[graph_tail_indcs].reshape(-1)\n",
        "\n",
        "    checksum_tail = (graph_tail_entities == data_tail_entities).sum().item()\n",
        "    if not checksum_head == step:\n",
        "      print('tail')\n",
        "      print(i)\n",
        "      break\n",
        "\n",
        "  else:\n",
        "    print('All clear :)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmd5ucYzDcqN",
        "cellView": "form"
      },
      "source": [
        "#@title partial_graph_data_maker(dataset, x_feature) FUNCTION{ form-width: \"15%\" }\n",
        "def partial_graph_data_maker(messaging: torch.Tensor, supervision: torch.Tensor, negative_samples: torch.Tensor, x_feature: str=Global.FEATURE_ENG.value, check_for_correctness: bool=False) -> Data:\n",
        "  relation_idx = Global.RELATION_INDEX.value\n",
        "  head_idx = Global.HEAD_INDEX.value\n",
        "  tail_idx = Global.TAIL_INDEX.value\n",
        "\n",
        "\n",
        "  if x_feature == 'one-hot':\n",
        "    one_hot_index = 0\n",
        "    edge_index_list_flat = []\n",
        "    seen_dict = defaultdict(lambda: -1)\n",
        "    edge_index_flat = messaging[:, (head_idx, tail_idx)].reshape(-1)\n",
        "\n",
        "    for i in range(edge_index_flat.shape[0]):\n",
        "      entity = edge_index_flat[i].item()\n",
        "      if seen_dict[entity] == -1:\n",
        "        edge_index_list_flat.append(one_hot_index)\n",
        "        seen_dict[entity] = one_hot_index\n",
        "        one_hot_index += 1\n",
        "      else:\n",
        "        edge_index_list_flat.append(seen_dict[entity])\n",
        "      if i + 1 % 50000 == 0:\n",
        "        print(f'Entity {i} encoded. {i / edge_index_flat.shape[0] * 100:.2f} %')\n",
        "    edge_index_messaging = torch.tensor(edge_index_list_flat).reshape(-1, 2)\n",
        "    ##############################################################\n",
        "    edge_index_flat = supervision[:, (head_idx, tail_idx)].reshape(-1)\n",
        "    edge_index_list_flat = []\n",
        "\n",
        "    for i in range(edge_index_flat.shape[0]):\n",
        "      entity = edge_index_flat[i].item()\n",
        "      if seen_dict[entity] == -1:\n",
        "        edge_index_list_flat.append(one_hot_index)\n",
        "        seen_dict[entity] = one_hot_index\n",
        "        one_hot_index += 1\n",
        "      else:\n",
        "        edge_index_list_flat.append(seen_dict[entity])\n",
        "      if i + 1 % 50000 == 0:\n",
        "        print(f'Entity {i} encoded. {i / edge_index_flat.shape[0] * 100:.2f} %')\n",
        "    edge_index_supervision = torch.tensor(edge_index_list_flat).reshape(-1, 2)\n",
        "    ##############################################################\n",
        "    edge_index_flat = negative_samples[:, (head_idx, tail_idx)].reshape(-1)\n",
        "    edge_index_list_flat = []\n",
        "\n",
        "    for i in range(edge_index_flat.shape[0]):\n",
        "      entity = edge_index_flat[i].item()\n",
        "      if seen_dict[entity] == -1:\n",
        "        edge_index_list_flat.append(one_hot_index)\n",
        "        seen_dict[entity] = one_hot_index\n",
        "        one_hot_index += 1\n",
        "      else:\n",
        "        edge_index_list_flat.append(seen_dict[entity])\n",
        "      if i + 1 % 50000 == 0:\n",
        "        print(f'Entity {i} encoded. {i / edge_index_flat.shape[0] * 100:.2f} %')\n",
        "    edge_index_negative = torch.tensor(edge_index_list_flat).reshape(-1, 2)\n",
        "    ##############################################################\n",
        "    x = torch.tensor(list(seen_dict.keys())).reshape(-1, 1)\n",
        "\n",
        "    graph_data = Data(\n",
        "        x=x,\n",
        "        edge_index_messaging=edge_index_messaging.t().contiguous(),\n",
        "        edge_type_messaging=messaging[:, relation_idx],\n",
        "        edge_index_supervision=edge_index_supervision.t().contiguous(),\n",
        "        edge_type_supervision=supervision[:, relation_idx],\n",
        "        edge_index_negative=edge_index_negative.t().contiguous(),\n",
        "        edge_type_negative=negative_samples[:, (relation_idx)]\n",
        "    )\n",
        "  else:\n",
        "    raise NotImplementedError('This functionality has not been implemented yet.')\n",
        "\n",
        "  if check_for_correctness:\n",
        "    checksum(dataset, graph_data)\n",
        "  return graph_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8jt1Q1RoExW",
        "cellView": "form"
      },
      "source": [
        "#@title visualize_graph(graph_data, height, width) FUNCTION{ form-width: \"15%\" }\n",
        "def visualize_graph(graph_data: Data, height: int=10, width:int=10) -> NoReturn:\n",
        "  nx_graph = to_networkx(graph_data)\n",
        "\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(width, height)\n",
        "\n",
        "  edge_labels = dict()\n",
        "  ei_np = graph_data.edge_index.t().numpy()\n",
        "  for edge in nx_graph.edges():\n",
        "    e = np.array(edge)\n",
        "    idx = np.where(ei_np == e)[0][0]\n",
        "    label = graph_data.edge_type[idx].item()\n",
        "    edge_labels.update({edge: label})\n",
        "    \n",
        "  nx.draw_networkx_nodes(nx_graph, pos)\n",
        "  nx.draw_networkx_edges(nx_graph, pos, connectionstyle='arc3,rad=0.2')\n",
        "  nx.draw_networkx_labels(nx_graph, pos, labels={n:graph_data.x[n].item() for n in nx_graph})\n",
        "  nx.draw_networkx_edge_labels(nx_graph, pos, edge_labels=edge_labels)\n",
        "\n",
        "  fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaarJu2fzJ68",
        "cellView": "form"
      },
      "source": [
        "#@title Visualization OPTIONAL-MAIN { form-width: \"15%\" }\n",
        "data_subset = train_set[:2500, :]\n",
        "graph_data = graph_data_maker(data_subset, 'one-hot')\n",
        "visualize_graph(graph_data, 100, 100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}